{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a6a3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style id=hide>div.input{display:none;}</style>\n",
       "<button type=\"button\"onclick=\"var myStyle = document.getElementById('hide').sheet;\n",
       "myStyle.insertRule('div.input{display:inherit !important;}', 0);\">Show the code</button>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run me to hide code cells\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(r\"\"\"<style id=hide>div.input{display:none;}</style>\n",
    "<button type=\"button\"onclick=\"var myStyle = document.getElementById('hide').sheet;\n",
    "myStyle.insertRule('div.input{display:inherit !important;}', 0);\">Show the code</button>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3b09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "'''\n",
    "This python file defines some functions that are used regually in this project.\n",
    "'''\n",
    "\n",
    "\n",
    "def read_text_file(file_name: str) -> str:\n",
    "    '''\n",
    "    This function is used to read the text file and return the content.\n",
    "    '''\n",
    "    with open(file_name, 'r+', encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def read_json_file(file_name: str):\n",
    "    '''\n",
    "    This function is used to read the json file and return the content.\n",
    "    '''\n",
    "    return json.loads(read_text_file(file_name))\n",
    "\n",
    "\n",
    "def download_text_file(url: str) -> str:\n",
    "    '''\n",
    "    This function is to use request to download the content into text format.\n",
    "    '''\n",
    "    return requests.get(url).text\n",
    "\n",
    "\n",
    "def download_json_file(url: str):\n",
    "    '''\n",
    "    This function is use request to download the content into json format.\n",
    "    '''\n",
    "    return requests.get(url).json()\n",
    "\n",
    "\n",
    "def write_file(file_name: str, content: str) -> str:\n",
    "    '''\n",
    "    This function is used to write the downloaded content in text or json format into files.\n",
    "    '''\n",
    "    # If the directory path does not exist, create one.\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    # If the file_name does exist, print 'the file has already been downloaded'.\n",
    "    if os.path.isfile(file_name):\n",
    "        print(f'{file_name} has already been downloaded')\n",
    "    # Otherwise, write the content into files and print 'the file has just been downloaded.'\n",
    "    else:\n",
    "        with open(file_name, 'w+', encoding='utf-8') as file:\n",
    "            print(f'{file_name} has just been downloaded.')\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b1e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from utils import read_json_file\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "'''\n",
    "Since the aim of this project is to investigate the necessary job skills required for each position. \n",
    "The fact refers to each job details that is posted as well as the fact table is utilized for storing a collection of measures \n",
    "such as section id, line id, job details, etc. \n",
    "The Job Info Dimensional Table contains all the dimensions of each job such as the job title, job company, job area, etc.\n",
    "The grain of fact table are job_id, section_id, and line_id which can be used to identify each job details. \n",
    "The grain of dimensional table is job_id which could be used to identify job information.\n",
    "\n",
    "Therefore, this python file is used for transforming the data into Job Details Fact table and Job Info Dimension Table.\n",
    "'''\n",
    "\n",
    "\n",
    "def remove_key(d: dict) -> dict:\n",
    "    '''\n",
    "    This function is to remove the specific key from the dictionary.\n",
    "    '''\n",
    "    d_copy = d.copy()\n",
    "    if d_copy.get('job_lines'):\n",
    "        d_copy.pop('job_lines')\n",
    "    return d_copy\n",
    "\n",
    "\n",
    "def transfer_job_info(jobs) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function is to remove the job_details from the data file and save the rest into job_info dataframe.\n",
    "    '''\n",
    "    job_info = [remove_key(job) for job in jobs]\n",
    "    return pd.DataFrame(job_info)\n",
    "\n",
    "\n",
    "def transfer_uls(jobs) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function is to save job_details into dataframe.\n",
    "    '''\n",
    "    data = []\n",
    "    for job in jobs:\n",
    "        uls = job.get('job_lines', [])\n",
    "        # Add section_index(section_id) and line_index(line_id) to every line of the job_details (uls)\n",
    "        for ul_idx, ul in enumerate(uls):\n",
    "            for li_idx, li in enumerate(ul):\n",
    "                # Append the job_id, section_id, line_id and detail into the data list.\n",
    "                data.append({\n",
    "                    'job_id': job.get('job_id'),\n",
    "                    'section_id': ul_idx + 1,\n",
    "                    'line_id': li_idx + 1,\n",
    "                    'detail': li\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "job_locations = ['Melbourne', 'Sydney', 'Brisbane', 'Perth',\n",
    "                'Adelaide', 'ACT', 'Hobart', 'Gold Coast']\n",
    "\n",
    "job_details = []\n",
    "\n",
    "Read all the json files into list job_details\n",
    "for job_location in job_locations:\n",
    "    for file in glob(f'downloads/seek/dataanalyst/{job_location}/*_details.json'):\n",
    "        job_details.append(read_json_file(file))\n",
    "\n",
    "\n",
    "Save the job_details including section_index, line_index to CSV file\n",
    "details_df = transfer_uls(job_details)\n",
    "details_df.to_csv('seek_data_analyst_job_details.csv', index=False)\n",
    "\n",
    "# Save the job_info Dimension Table into CSV file including the rest information such as job title, job company, etc.\n",
    "info_df = transfer_job_info(job_details)\n",
    "info_df.to_csv('seek_data_analyst_job_info.csv', index=False)\n",
    "\n",
    "# Conncet to the SQLite3 and tranfer the data into SQL files\n",
    "con = sqlite3.connect('data_analyst_jobs_details.sqlite3')\n",
    "details_df.to_sql('fact_job_details', con=con, index=False, if_exists='replace')\n",
    "\n",
    "con = sqlite3.connect('data_analyst_jobs_info.sqlite3')\n",
    "info_df.to_sql('dim_job_info', con=con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b245c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "'''\n",
    "This python is used for job_salary data cleaning even though the objectives of this project \n",
    "do not include the job salary analysis.\n",
    "'''\n",
    "\n",
    "# Read the csv file into pandas dataframe.\n",
    "job_details = pd.read_csv('seek_data_analyst_job_info.csv')\n",
    "\n",
    "# job_salary data cleaning\n",
    "# Create two empty lsit for the new columns. \n",
    "# salary_down means the lower limit of the salary from the job_info.\n",
    "salary_down = []\n",
    "# salary_up means the upper limit of the salary from the job_info.\n",
    "salary_up = []\n",
    "\n",
    "for i in job_details['job_salary']:\n",
    "    # Use Regular Expression to extract the salary data and transform them into float numbers from the database.\n",
    "    salary = re.findall(r'[\\$-]([\\d, kK]+)', str(i))\n",
    "    salary_new1 = [s.replace(',', '') for s in salary]\n",
    "    salary_new2 = [s.replace('k', '000') for s in salary_new1]\n",
    "    salary_new3 = [s.replace('K', '000') for s in salary_new2]\n",
    "    # Remove the blank\n",
    "    salary_new4 = [s.replace(' ', '') for s in salary_new3]\n",
    "    # Remove the null value\n",
    "    salary_new5 = [s for s in salary_new4 if s != '']\n",
    "    \n",
    "    # Append the transformed data into the list.\n",
    "    # If there is only one salary value, append it into the salary_down list.\n",
    "    if len(salary_new5) == 1 and salary_new5[0].isdigit():\n",
    "        salary_down.append(float(salary_new5[0]))\n",
    "        salary_up.append('')\n",
    "    # If there are two salary values, append them into the salary_down and salary_up list separately.\n",
    "    elif len(salary_new5) == 2:\n",
    "        salary_down.append(float(salary_new5[0]))\n",
    "        salary_up.append(float(salary_new5[1]))\n",
    "    # If there is no salary information, append null value into the lists.\n",
    "    else:\n",
    "        salary_down.append('')\n",
    "        salary_up.append('')\n",
    "\n",
    "# Create two new columns to the job_details dataframe.\n",
    "job_details['job_salary_down'] = salary_down\n",
    "job_details['job_salary_up'] = salary_up\n",
    "\n",
    "# Based on different salary range, the job salary can be divided into following three categories: \n",
    "# If salary is lower than 200, it is paid by per hour;\n",
    "# If salary is greater than 200 and lower than 2000, it is paid by per day;\n",
    "# If salary is greater than 2000, it is paid by per annual.\n",
    "# In such way, the annual salary can be calculated according to the full-time working hours from Australian government website. \n",
    "# There are 251 working days or 2008 working hours per year for a full-time working position.\n",
    "\n",
    "job_details['annual_salary_down'] = job_details['job_salary_down']\n",
    "\n",
    "job_details.annual_salary_down[(200 > job_details['job_salary_down'])] = job_details['job_salary_down'] * 2008\n",
    "job_details.annual_salary_down[(200 < job_details['job_salary_down'])\n",
    "                          & (job_details['job_salary_down'] < 2000)] = job_details['job_salary_down'] * 251\n",
    "\n",
    "\n",
    "job_details['annual_salary_up'] = job_details['job_salary_up']\n",
    "\n",
    "job_details.annual_salary_up[(200 > job_details['job_salary_down'])] = job_details['job_salary_up'] * 2008\n",
    "job_details.annual_salary_up[(200 < job_details['job_salary_down'])\n",
    "                          & (job_details['job_salary_down'] < 2000)] = job_details['job_salary_up'] * 251\n",
    "\n",
    "# Finally, the data can be converted into a CSV file and needs to be manually checked for errors. \n",
    "job_details.to_csv('seek_data_analyst_job_info_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70116b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "'''\n",
    "This python file is used to transfrom datetime data into consistent data formats.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def getdate(days_ago: int, hours_ago: int, minutes_ago: int):\n",
    "    '''\n",
    "    This function is used to transform the datetime to '%Y-%m-%d %H:%M:%S' data format.\n",
    "    '''\n",
    "    time_now = datetime.utcnow()\n",
    "    time_diff = timedelta(days=days_ago, hours=hours_ago, minutes=minutes_ago)\n",
    "    return (time_now - time_diff).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# Read the CSV file into pandas dataframe\n",
    "job_details = pd.read_csv('seek_data_analyst_job_info_cleaned.csv')\n",
    "# Create a new list for the cleaned and transformed job_posted_time\n",
    "job_posted_time = []\n",
    "\n",
    "'''\n",
    "The job Ad posted time usually have the following suffixes:\n",
    "1. 'm' means the job ad was posted minutes ago; \n",
    "2. 'h' means the job ad was posted hours before; \n",
    "3. 'd' means the job ad was posted days ago.\n",
    "Based on these suffixes, we can use the Python datetime library \n",
    "to calculate the Universal Time Coordinated date of the job ad posted time.\n",
    "'''\n",
    "for dates in job_details['job_listing_date']:\n",
    "    if 'd' in str(dates):\n",
    "        days_ago = int(dates.split('d')[0])\n",
    "        job_posted_time.append(getdate(days_ago, 0, 0))\n",
    "    elif 'h' in str(dates):\n",
    "        hours_ago = int(dates.split('h')[0])\n",
    "        job_posted_time.append(getdate(0, hours_ago, 0))\n",
    "    elif 'm' in str(dates):\n",
    "        minutes_ago = int(dates.split('m')[0])\n",
    "        job_posted_time.append(getdate(0, 0, minutes_ago))\n",
    "    else:\n",
    "        job_posted_time.append('')\n",
    "\n",
    "# Create new column to the job_posted_time dataframe.\n",
    "job_details['job_posted_time'] = job_posted_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c99ad",
   "metadata": {},
   "source": [
    "#### Filter the Related Recruitment Data for Data Analyst, Data Engineer, Data Scientist from More Than 10,000 Job Descriptions.\n",
    "\n",
    "The search algorithm from SEEK website brings up lots of irrelevant job titles, such as business intelligence, accountant, etc. Therefore, before further analysis, we need to filter out the related recruitment data for Data Analyst, Data Engineer, Data Scientist. In this project, we use SQL LIKE and UNLIKE operator to identify whether if the job title belongs to these three titles or not.\n",
    "Finally, from 16,000 job advertisements, there are 2995 job advertisements for Data Analyst, 1426 job advertisements for Data Engineer, and 729 job advertisements for Data Scientist. \n",
    "\n",
    "---\n",
    "\n",
    "```sql\n",
    "WITH \n",
    "enriched AS (\n",
    "\tSELECT job_id, job_title\n",
    "\t\t\t, CASE \n",
    "\t\t\tWHEN job_title LIKE '%data%analyst%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title LIKE '%bi%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title LIKE '%tableau%'  THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%sql%' THEN 'data analyst'\t\t\n",
    "\t\t\tWHEN job_title LIKE '%analytic%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%data%modeller%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%visualisation%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%business%intelligence%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title like '%insight%' THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%reporting%' THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%model%' THEN 'data analyst'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%data%engineer%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%warehouse%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%architect%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%snowflake%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%etl%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%api%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%cloud%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%aws%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%kafka%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%pipeline%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%migration%' THEN 'data engineer'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%scientist%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%science%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%machine%learning%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '% ai%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%ai %' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%ml%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%computational%statistics%' then 'data scientist'\n",
    "\t\t\tWHEN job_title like '%quantitative%' then 'data scientist'\n",
    "\t\t\tWHEN job_title like '%artificial%intelligence%' then 'data scientist'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%business%analyst%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%accountant%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%sales%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%product%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%dev%ops%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%graduate%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%developer%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%functional%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%financial%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%finance%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%manager%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%project%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%program%' THEN 'else'\n",
    "\t\t\t\n",
    "\t\t\tEND AS job_standard_name\n",
    "\tFROM dim_job_info\n",
    "\tORDER BY 2 DESC\n",
    "\t)\n",
    "\n",
    ", filtered_job_details AS (\n",
    "\tSELECT *\n",
    "\tFROM dim_job_details\n",
    "\tWHERE job_id IN (\n",
    "\t\tSELECT job_id FROM enriched \n",
    " \t\tWHERE job_standard_name = 'data analyst' )\n",
    "-- \t\tWHERE job_standard_name = 'data engineer' )\n",
    "--\t\tWHERE job_standard_name = 'data scientist' )\n",
    "\t)\n",
    "\n",
    "-- Select the job_details of each job_standard_name and transfrom them into CSV files\n",
    "\n",
    "SELECT *\n",
    "FROM filtered_job_details\n",
    "\n",
    "-- Count the JD numbers of each job_standard_name\n",
    "SELECT job_standard_name, count(1)\n",
    "FROM enriched\n",
    "GROUP BY 1\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Iterable, Any\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "'''\n",
    "From the filtered JDs above, this python file is used to calculate the appeared frequency of each word and phrase. \n",
    "Then we can filter out the meaningful word and phrase with high appeared frequency including technical and soft skills.\n",
    "'''\n",
    "\n",
    "# Download the stopword from nltk library.\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def ngram(l: Iterable[Any], gram: int) -> List[Any]:\n",
    "    '''\n",
    "    Return the phrases with length of gram from the list.\n",
    "    '''\n",
    "    l = list(l)\n",
    "    return [l[i: i + gram] for i in range(len(l) - gram + 1)]\n",
    "\n",
    "\n",
    "def get_words(line: str, gram: int) -> List[str]:\n",
    "    '''\n",
    "    This function is to return words or phrases with defined length.    \n",
    "    '''\n",
    "    words = re.findall(r'[a-zA-Z]+', line.lower())\n",
    "    # If gram equals to one, filter out the stop_words and return the other words.\n",
    "    if gram == 1:\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    # Otherwise, return the Return the phrases with length of gram from the sentence.\n",
    "    else:\n",
    "        words = [' '.join(word) for word in ngram(words, gram)]\n",
    "    return words\n",
    "\n",
    "\n",
    "def word_count(stats: dict, sentence: str, gram: int) -> dict:\n",
    "    '''\n",
    "    Return the calculated appeared frequency of each word and phrase from each line of the job_detail. \n",
    "    '''\n",
    "    if isinstance(sentence, str):\n",
    "        for word in get_words(sentence, gram):\n",
    "            # If the word already exisits in the dict, the value pluses one.\n",
    "            if word in stats:\n",
    "                stats[word] += 1\n",
    "            # Otherwise, append the word with value one to the dict.\n",
    "            else:\n",
    "                stats[word] = 1\n",
    "    return stats\n",
    "    \n",
    "\n",
    "def sorted_word_count(d: dict) -> Tuple:\n",
    "    '''\n",
    "    This function is used to sort the word count dictionary with values in reverse order.\n",
    "    '''\n",
    "    return sorted(list(d.items()), key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "\n",
    "# read the job details file\n",
    "wc = {}\n",
    "job_uls = pd.read_csv('seek_data_analyst_job_details.csv')\n",
    "\n",
    "# For each line of the job_details, count the appeared frequency of the word or phrases.\n",
    "for ul in job_uls['detail']:\n",
    "    wc = word_count(wc, ul, gram=1)\n",
    "\n",
    "# Sort the word count dictionary 'wc' with values in reverse order.\n",
    "for i in sorted_word_count(wc):\n",
    "    print(i)\n",
    "\n",
    "# Transform the sorted result and save it into CSV file for further analysis to filter out the meaningful words.\n",
    "df = pd.DataFrame(sorted_word_count(wc))\n",
    "df.to_csv('seek_data_scientist_gram_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Iterable, Any\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "'''\n",
    "After the analysis above, we filter out 46 highly-demand technical and 12 soft skills in the job market \n",
    "as shown in the keywords list.\n",
    "Then we can use Power BI to draw the word cloud and caculated the averaged line_id of each skill.\n",
    "'''\n",
    "\n",
    "\n",
    "def filtered_keywords(sentence: str) -> list:\n",
    "    '''\n",
    "    This function is to filter out the skills from each sentence.\n",
    "    '''\n",
    "    result = []\n",
    "    if isinstance(sentence, str):\n",
    "        for word in keywords:\n",
    "            if word.lower() in sentence.lower():\n",
    "                result.append(word)\n",
    "    return result\n",
    "\n",
    "\n",
    "def ngram(l: Iterable[Any], gram: int) -> List[Any]:\n",
    "    '''\n",
    "    Return the phrases with length of gram from the list.\n",
    "    '''\n",
    "    l = list(l)\n",
    "    return [l[i: i + gram] for i in range(len(l) - gram + 1)]\n",
    "    \n",
    "    \n",
    "def get_words(line: str, gram: int) -> List[str]:\n",
    "    '''\n",
    "    This function is to return words or phrases with defined length.    \n",
    "    '''\n",
    "    words = re.findall(r'[a-zA-Z]+', line.lower())\n",
    "    # If gram equals to one, filter out the stop_words and return the other words.\n",
    "    if gram == 1:\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    # Otherwise, return the Return the phrases with length of gram from the sentence.\n",
    "    else:\n",
    "        words = [' '.join(word) for word in ngram(words, gram)]\n",
    "    return words\n",
    "\n",
    "    \n",
    "keywords = ['SQL', 'Excel', 'PowerBI', 'Tableau', 'Python', 'Cloud'\n",
    "        , 'Azure', 'AWS', 'GCP', 'API', 'Pipeline', 'Dimension Modelling', 'ETL', 'ELT'\n",
    "        , 'DevOps', 'CI CD', 'Spark', 'Java', 'Scala', 'Oracle', 'Kubernetes', 'Docker'\n",
    "        , 'Apache', 'Kafka', 'Linux', 'Snowflake', 'Data Warehouse'\n",
    "        , 'Data Modelling', 'Data Visualisation', 'Data Migration', 'Data Management'\n",
    "        , 'Data Integration', 'Data Platform', 'Data Architecture', 'Data Factory', 'Databricks', 'Data Science'\n",
    "        , 'Machine Learning', 'Computer Science', 'Research', 'Statistic', 'Mathematics'\n",
    "        , 'Quantitative', 'Algorithm', 'Deep Learning', 'Statistical Analysis'\n",
    "        , 'Communication Skill', 'Stakeholder', 'Reporting', 'Agile'\n",
    "        , 'Project Management', 'Business Intelligence', 'Decision Making', 'Interpersonal'\n",
    "        , 'Time Management', 'Troubleshoot', 'Tertiary Qualification', 'PhD']\n",
    "    \n",
    "\n",
    "# read the job details file\n",
    "exsited_keyword = []\n",
    "job_uls = pd.read_csv('seek_data_analyst_job_details.csv)\n",
    "\n",
    "# filter out 46 highly-demand technical and 12 soft skills in the job market as shown in the keywords list.                      \n",
    "for ul in job_uls['detail']:\n",
    "    exsited_keyword.append(filtered_keywords(ul))\n",
    "\n",
    "job_uls['exsited_keyword'] = exsited_keyword\n",
    "# Save the result into CSV file. Then we can use Power BI to draw the word cloud.                      \n",
    "job_uls.to_csv('seek_data_analyst_filtered_keywords.csv', index=False)                  \n",
    "                      \n",
    "# Create the dictionary to caculate the averaged line_id of each skill.\n",
    "keywords_dict = {'SQL':[], 'Excel':[], 'Power BI':[], 'Tableau':[], 'Python':[], 'Cloud':[], 'Azure':[]\n",
    "        , 'AWS':[], 'GCP':[], 'API':[], 'Pipeline':[], 'Dimension Modelling':[], 'ETL':[], 'ELT':[]\n",
    "        , 'DevOps':[], 'CI CD':[], 'Spark':[], 'Java':[], 'Scala':[], 'Oracle': [], 'Kubernetes': []\n",
    "        , 'Docker':[], 'Apache': [], 'Kafka':[], 'Linux':[], 'Snowflake':[], 'Data Warehouse':[]\n",
    "        , 'Data Modelling':[], 'Data Visualisation':[], 'Data Migration':[], 'Data Management':[]\n",
    "        , 'Data Integration':[], 'Data Platform':[], 'Data Architecture':[], 'Data Factory':[], 'Data Science':[]\n",
    "        , 'Machine Learning':[], 'Computer Science':[], 'Research':[], 'Statistic':[], 'Mathematics':[]\n",
    "        , 'Quantitative':[], 'Algorithm':[], 'Deep Learning':[], 'Statistical Analysis':[]\n",
    "        , 'Communication Skill':[], 'Stakeholder': [], 'Reporting':[], 'Agile':[]\n",
    "        , 'Project Management':[], 'Business Intelligence':[], 'Decision Making':[], 'Interpersonal':[]\n",
    "        , 'Time Management':[], 'Troubleshoot':[], 'Tertiary Qualification':[], 'PhD':[]}\n",
    "                                          \n",
    "for index, row in job_uls.iterrows():\n",
    "    if isinstance(row['detail'], str):\n",
    "        for word in get_words(row['detail'], 1):\n",
    "            for skill in keywords:\n",
    "                if word == skill.lower():\n",
    "                    keywords_dict[skill].append(row['line_id'])\n",
    "        for word in get_words(row['detail'], 2):\n",
    "            for skill in keywords:\n",
    "                if word == skill.lower():\n",
    "                    keywords_dict[skill].append(row['line_id'])\n",
    "\n",
    "skill_ranked_line = {}\n",
    "                      \n",
    "for key, value in keywords_dict.items():\n",
    "    if len(value) != 0:\n",
    "        skill_ranked_line[key] = sum(value) / len(value)\n",
    "\n",
    "skill_ranked_line.to_csv('skill_ranked_line.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601eae42",
   "metadata": {},
   "source": [
    "#### Documentary Frequency Analysis and Skills Required to be a Data Analyst, Data Engineer and Data Scientist.\n",
    "\n",
    "After we filter out the important skills, we can use SQL LIKE operator to identify which statements contain these keywords and count the distinct total number of job ads.\n",
    "\n",
    "---\n",
    "\n",
    "```sql\n",
    "WITH \n",
    "enriched AS (\n",
    "\tSELECT job_id, job_title\n",
    "\t\t\t, CASE \n",
    "\t\t\tWHEN job_title LIKE '%data%analyst%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title LIKE '%bi%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title LIKE '%tableau%'  THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%sql%' THEN 'data analyst'\t\t\n",
    "\t\t\tWHEN job_title LIKE '%analytic%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%data%modeller%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%visualisation%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%business%intelligence%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title like '%insight%' THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%reporting%' THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%model%' THEN 'data analyst'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%data%engineer%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%warehouse%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%architect%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%snowflake%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%etl%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%api%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%cloud%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%aws%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%kafka%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%pipeline%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%migration%' THEN 'data engineer'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%scientist%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%science%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%machine%learning%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '% ai%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%ai %' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%ml%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%computational%statistics%' then 'data scientist'\n",
    "\t\t\tWHEN job_title like '%quantitative%' then 'data scientist'\n",
    "\t\t\tWHEN job_title like '%artificial%intelligence%' then 'data scientist'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%business%analyst%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%accountant%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%sales%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%product%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%dev%ops%' THEN 'else'\n",
    "\t\t\tWHEN job_title like '%graduate%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%developer%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%functional%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%financial%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%finance%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%manager%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%project%' THEN 'else'\n",
    "\t\t\tWHEN job_title LIKE '%program%' THEN 'else'\n",
    "\t\t\t\n",
    "\t\t\tEND AS job_standard_name\n",
    "\tFROM dim_job_info\n",
    "\tORDER BY 2 DESC\n",
    "\t)\n",
    "\n",
    ", filtered_job_details AS (\n",
    "\tSELECT *\n",
    "\tFROM dim_job_details\n",
    "\tWHERE job_id IN (\n",
    "\t\tSELECT job_id FROM enriched \n",
    " \t\tWHERE job_standard_name = 'data analyst' )\n",
    "-- \t\tWHERE job_standard_name = 'data engineer' )\n",
    "--\t\tWHERE job_standard_name = 'data scientist' )\n",
    "\t)\n",
    "\n",
    ", enriched_details AS (\n",
    "\tSELECT *\n",
    "\t\t, CASE WHEN detail LIKE '%sql%' THEN '1' END AS sql_counter\n",
    "\t\t, CASE WHEN detail LIKE '%excel%' THEN '1' END AS excel_counter\n",
    "\t\t, CASE WHEN detail LIKE '%power%bi%' THEN '1' END AS powerbi_counter\n",
    "\t\t, CASE WHEN detail LIKE '%tableau%' THEN '1' END AS tableau_counter\n",
    "\t\t, CASE WHEN detail LIKE '%python%' THEN '1' END AS python_counter\n",
    "\t\t, CASE WHEN detail LIKE '%cloud%' THEN '1' END AS cloud_counter\n",
    "\t\t, CASE WHEN detail LIKE '%azure%' THEN '1' END AS azure_counter\n",
    "\t\t, CASE WHEN detail LIKE '%aws%' THEN '1' END AS aws_counter\n",
    "\t\t, CASE WHEN detail LIKE '%gcp%' THEN '1' END AS gcp_counter\n",
    "\t\t\n",
    "\t\t, CASE WHEN detail LIKE '%api%' THEN '1' END AS api_counter\n",
    "\t\t, CASE WHEN detail LIKE '%pipeline%' THEN '1' END AS pipeline_counter\n",
    "\t\t, CASE WHEN detail LIKE '%dimension%' THEN '1' END AS dimension_counter\n",
    "\t\t, CASE WHEN detail LIKE '%etl%' THEN '1' END AS etl_counter\n",
    "\t\t, CASE WHEN detail LIKE '%elt%' THEN '1' END AS elt_counter\n",
    "\t\t, CASE WHEN detail LIKE '%devops%' THEN '1' END AS devops_counter\n",
    "\t\t, CASE WHEN detail LIKE '%ci%cd%' THEN '1' END AS cicd_counter\n",
    "\t\t, CASE WHEN detail LIKE '%spark%' THEN '1' END AS spark_counter\n",
    "\t\t, CASE WHEN detail LIKE '%java%' THEN '1' END AS java_counter\n",
    "\t\t, CASE WHEN detail LIKE '%scala%' THEN '1' END AS scala_counter\n",
    "\t\t, CASE WHEN detail LIKE '%kafka%' THEN '1' END AS kafka_counter\n",
    "\t\t, CASE WHEN detail LIKE '%linux%' THEN '1' END AS linux_counter\n",
    "\t\t, CASE WHEN detail LIKE '%snowflake%' THEN '1' END AS snowflake_counter\t\n",
    "\t\t, CASE WHEN detail LIKE '%oracle%' THEN '1' END AS oracle_counter\n",
    "\t\t, CASE WHEN detail LIKE '%kubernetes%' THEN '1' END AS kubernetes_counter\n",
    "\t\t, CASE WHEN detail LIKE '%docker%' THEN '1' END AS docker_counter\n",
    "\t\t, CASE WHEN detail LIKE '%apache%' THEN '1' END AS apache_counter\n",
    "\t\t\n",
    "\t\t, CASE WHEN detail LIKE '%data%warehous%' THEN '1' END AS DW_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%modelling%' THEN '1' END AS DM_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%visualisation%' THEN '1' END AS DV_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%migration%' THEN '1' END AS DMI_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%management%' THEN '1' END AS DMA_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%integration%' THEN '1' END AS DI_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%platform%' THEN '1' END AS DP_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%architecture%' THEN '1' END AS DA_counter\n",
    "\t\t, CASE WHEN detail LIKE '%data%factory%' THEN '1' END AS DF_counter\n",
    "\t\t, CASE WHEN detail LIKE '%databricks%' THEN '1' END AS DB_counter\n",
    "\t\t\n",
    "\t\t, CASE WHEN detail LIKE '%data%science%' THEN '1' END AS DS_counter\n",
    "\t\t, CASE WHEN detail LIKE '%machine%learning%' THEN '1' END AS ML_counter\n",
    "\t\t, CASE WHEN detail LIKE '% ai %' THEN '1' END AS AI_counter\n",
    "\t\t, CASE WHEN detail LIKE '%computer%science%' THEN '1' END AS CS_counter\n",
    "\t\t, CASE WHEN detail LIKE '%research%' THEN '1' END AS research_counter\n",
    "\t\t, CASE WHEN detail LIKE '%statistic%' THEN '1' END AS statistic_counter\n",
    "\t\t, CASE WHEN detail LIKE '%mathematics%' THEN '1' END AS mathematics_counter\n",
    "\t\t, CASE WHEN detail LIKE '%quantitative%' THEN '1' END AS quantitative_counter\n",
    "\t\t, CASE WHEN detail LIKE '%algorithm%' THEN '1' END AS algorithm_counter\n",
    "\t\t, CASE WHEN detail LIKE '%deep%learning%' THEN '1' END AS DL_counter\n",
    "\t\t, CASE WHEN detail LIKE '%statistical%analysis%' THEN '1' END AS SA_counter\n",
    "        \n",
    "        , CASE WHEN detail LIKE '%communication%' THEN '1' END AS communication_counter\n",
    "\t\t, CASE WHEN detail LIKE '%reporting%' THEN '1' END AS reporting_counter\n",
    "\t\t, CASE WHEN detail LIKE '%agile%' THEN '1' END AS agile_counter\n",
    "\t\t\n",
    "\t\t, CASE WHEN detail LIKE '%stakeholder%' THEN '1' END AS stakeholder_counter\n",
    "\t\t, CASE WHEN detail LIKE '%project%management%' THEN '1' END AS PM_counter\n",
    "\t\t, CASE WHEN detail LIKE '%decision%making%' THEN '1' END AS DM_counter\n",
    "\t\t, CASE WHEN detail LIKE '%interpersonal%skills%' THEN '1' END AS IS_counter\n",
    "\t\t, CASE WHEN detail LIKE '%time%management%' THEN '1' END AS TM_counter\n",
    "\t\t, CASE WHEN detail LIKE '%troubleshoot%' THEN '1' END AS troubleshoot_counter\n",
    "\t\t, CASE WHEN detail LIKE '%tertiary%qualification%' THEN '1' END AS TQ_counter\n",
    "\t\t, CASE WHEN detail LIKE '%phd%' THEN '1' END AS phd_counter\n",
    "\t\t, CASE WHEN detail LIKE '%business%intelligence%' THEN '1' END AS BI_counter\n",
    "\n",
    "\tFROM filtered_job_details\n",
    "\t)\n",
    ", job_details AS (\n",
    "\tSELECT job_id\n",
    "\t\t, SUM(sql_counter) > 0 AS has_sql\n",
    "\t\t, SUM(excel_counter) > 0 AS has_excel\n",
    "\t\t, SUM(powerbi_counter) > 0 AS has_powerbi\n",
    "\t\t, SUM(tableau_counter) > 0 AS has_tableau\n",
    "\t\t, SUM(python_counter) > 0 AS has_python\n",
    "\t\t, SUM(cloud_counter) > 0 AS has_cloud\n",
    "\t\t, SUM(azure_counter) > 0 AS has_azure\n",
    "\t\t, SUM(aws_counter) > 0 AS has_aws\n",
    "\t\t, SUM(gcp_counter) > 0 AS has_gcp\n",
    "\t\t\n",
    "\t\t, SUM(api_counter) > 0 AS has_api\n",
    "\t\t, SUM(pipeline_counter) > 0 AS has_pipeline\n",
    "\t\t, SUM(dimension_counter) > 0 AS has_dimension\n",
    "\t\t, SUM(etl_counter) > 0 AS has_etl\n",
    "\t\t, SUM(elt_counter) > 0 AS has_elt\n",
    "\t\t, SUM(devops_counter) > 0 AS has_devops\n",
    "\t\t, SUM(cicd_counter) > 0 AS has_cicd\n",
    "\t\t, SUM(spark_counter) > 0 AS has_spark\n",
    "\t\t, SUM(java_counter) > 0 AS has_java\n",
    "\t\t, SUM(scala_counter) > 0 AS has_scala\n",
    "\t\t, SUM(kafka_counter) > 0 AS has_kafka\n",
    "\t\t, SUM(linux_counter) > 0 AS has_linux\n",
    "\t\t, SUM(snowflake_counter) > 0 AS has_snowflake\n",
    "\t\t, SUM(oracle_counter) > 0 AS has_oracle\n",
    "\t\t, SUM(kubernetes_counter) > 0 AS has_kubernetes\n",
    "\t\t, SUM(docker_counter) > 0 AS has_docker\n",
    "\t\t, SUM(apache_counter) > 0 AS has_apache\n",
    "\n",
    "\t\t, SUM(DW_counter) > 0 AS has_DW\n",
    "\t\t, SUM(DM_counter) > 0 AS has_DM\n",
    "\t\t, SUM(DV_counter) > 0 AS has_DV\n",
    "\t\t, SUM(DMI_counter) > 0 AS has_DMI\n",
    "\t\t, SUM(DMA_counter) > 0 AS has_DMA\n",
    "\t\t, SUM(DI_counter) > 0 AS has_DI\n",
    "\t\t, SUM(DP_counter) > 0 AS has_DP\n",
    "\t\t, SUM(DA_counter) > 0 AS has_DA\n",
    "\t\t, SUM(DF_counter) > 0 AS has_DF\n",
    "\t\t, SUM(DB_counter) > 0 AS has_DB\n",
    "\t\t\n",
    "\t\t, SUM(DS_counter) > 0 AS has_DS\n",
    "\t\t, SUM(ML_counter) > 0 AS has_ML\n",
    "\t\t, SUM(AI_counter) > 0 AS has_AI\n",
    "\t\t, SUM(CS_counter) > 0 AS has_CS\n",
    "\t\t, SUM(research_counter) > 0 AS has_research\n",
    "\t\t, SUM(statistic_counter) > 0 AS has_statistic\n",
    "\t\t, SUM(mathematics_counter) > 0 AS has_mathematics\n",
    "\t\t, SUM(quantitative_counter) > 0 AS has_quantitative\n",
    "\t\t, SUM(algorithm_counter) > 0 AS has_algorithm\n",
    "\t\t, SUM(DM_counter) > 0 AS has_DM\n",
    "\t\t, SUM(SA_counter) > 0 AS has_SA\n",
    "        \n",
    "        , SUM(communication_counter) > 0 AS has_communication\n",
    "\t\t, SUM(reporting_counter) > 0 AS has_reporting\n",
    "\t\t, SUM(agile_counter) > 0 AS has_agile\n",
    "\t\t, SUM(stakeholder_counter) > 0 AS has_stakeholder\n",
    "\t\t, SUM(PM_counter) > 0 AS has_PM\n",
    "\t\t, SUM(DM_counter) > 0 AS has_DM\n",
    "\t\t, SUM(IS_counter) > 0 AS has_IS\n",
    "\t\t, SUM(TM_counter) > 0 AS has_TM\n",
    "\t\t, SUM(troubleshoot_counter) > 0 AS has_troubleshoot\n",
    "\t\t, SUM(TQ_counter) > 0 AS has_TQ\n",
    "\t\t, SUM(phd_counter) > 0 AS has_phd\n",
    "\t\t, SUM(BI_counter) > 0 AS has_BI\n",
    "\n",
    "\tFROM enriched_details\n",
    "\tGROUP BY job_id\n",
    "\t)\n",
    "\n",
    "SELECT \n",
    "(SELECT COUNT(DISTINCT job_id) FROM filtered_job_details) AS total_jobs\n",
    "\t, SUM(has_sql) AS sql_jobs\n",
    "\t, SUM(has_excel) AS excel_jobs\n",
    "\t, SUM(has_powerbi) AS powerbi_jobs\n",
    "\t, SUM(has_tableau) AS tableau_jobs\n",
    "\t, SUM(has_python) AS python_jobs\n",
    "\t, SUM(has_cloud) AS cloud_jobs\n",
    "\t, SUM(has_azure) AS azure_jobs\n",
    "\t, SUM(has_aws) AS aws_jobs\n",
    "\t, SUM(has_gcp) AS gcp_jobs\n",
    "\t\n",
    "\t, SUM(has_api) AS api_jobs\n",
    "\t, SUM(has_pipeline) AS pipeline_jobs\n",
    "\t, SUM(has_dimension) AS dimension_jobs\n",
    "\t, SUM(has_etl) AS etl_jobs\n",
    "\t, SUM(has_elt) AS elt_jobs\n",
    "\t, SUM(has_devops) AS devops_jobs\n",
    "\t, SUM(has_cicd) AS cicd_jobs\n",
    "\t, SUM(has_spark) AS spark_jobs\n",
    "\t, SUM(has_java) AS java_jobs\n",
    "\t, SUM(has_scala) AS scala_jobs\n",
    "\t, SUM(has_kafka) AS kafka_jobs\n",
    "\t, SUM(has_linux) AS linux_jobs\n",
    "\t, SUM(has_snowflake) AS snowflake_jobs\n",
    "\t, SUM(has_oracle) AS oracle_jobs\n",
    "\t, SUM(has_kubernetes) AS kubernetes_jobs\n",
    "\t, SUM(has_docker) AS docker_jobs\n",
    "\t, SUM(has_apache) AS apache_jobs\n",
    "\n",
    "\t, SUM(has_DW) AS datawarehouse_jobs\n",
    "\t, SUM(has_DM) AS datamodelling_jobs\n",
    "\t, SUM(has_DV) AS datavisualisation_jobs\n",
    "\t, SUM(has_DMI) AS datamigration_jobs\n",
    "\t, SUM(has_DMA) AS datamanagement_jobs\n",
    "\t, SUM(has_DI) AS dataintegration_jobs\n",
    "\t, SUM(has_DP) AS dataplatform_jobs\n",
    "\t, SUM(has_DA) AS dataarchitecture_jobs\n",
    "\t, SUM(has_DF) AS datafactory_jobs\n",
    "\t, SUM(has_DB) AS databricks_jobs\n",
    "\t\n",
    "\t, SUM(has_DS) AS datascience_jobs\n",
    "\t, SUM(has_ML) AS machinelearning_jobs\n",
    "\t, SUM(has_AI) AS Artificialintelligence_jobs\n",
    "\t, SUM(has_CS) AS computerscience_jobs\n",
    "\t, SUM(has_research) AS research_jobs\n",
    "\t, SUM(has_statistic) AS statistic_jobs\n",
    "\t, SUM(has_mathematics) AS mathematics_jobs\n",
    "\t, SUM(has_quantitative) AS quantitative_jobs\n",
    "\t, SUM(has_algorithm) AS algorithm_jobs\n",
    "\t, SUM(has_DM) AS deeplearning_jobs\n",
    "\t, SUM(has_SA) AS statisticalanalysis_jobs\n",
    "    \n",
    "    , SUM(has_communication) AS Communication_Skill\n",
    "\t, SUM(has_reporting) AS Reporting\n",
    "\t, SUM(has_agile) AS Agile\n",
    "\t, SUM(has_stakeholder) AS Stakeholder\n",
    "\t, SUM(has_PM) AS Project_Management\n",
    "\t, SUM(has_BI) AS Business_Intelligence\n",
    "\t, SUM(has_DM) AS Decision_Making\n",
    "\t, SUM(has_IS) AS Interpersonal_Skill\n",
    "\t, SUM(has_TM) AS Time_Management\n",
    "\t, SUM(has_troubleshoot) AS Troubleshooting\n",
    "\t, SUM(has_TQ) AS Tertiary_Qualification\n",
    "\t, SUM(has_phd) AS PhD\n",
    "    \n",
    "\t\n",
    "FROM job_details\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e0028",
   "metadata": {},
   "source": [
    "**Ranked Line in the Job Description of the Skills**\n",
    "\n",
    "In order to illustrate further, we assume that people always would like to put the most important information first, for example, the higher the position in the job advertisement. Then we analyze the average appeared line number of these important skills. We categorize their first occurrence line numbers into three range: 1~3, 4~5 and 6+,\n",
    "\n",
    "---\n",
    "\n",
    "```sql\n",
    "\n",
    "WITH \n",
    "enriched AS (\n",
    "\tSELECT job_id, job_title\n",
    "\t\t\t, CASE \n",
    "\t\t\tWHEN job_title LIKE '%data%analyst%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title LIKE '%bi%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title LIKE '%tableau%'  THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%sql%' THEN 'data analyst'\t\t\n",
    "\t\t\tWHEN job_title LIKE '%analytic%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%data%modeller%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%visualisation%'  THEN 'data analyst' \n",
    "\t\t\tWHEN job_title LIKE '%business%intelligence%'  THEN 'data analyst'  \n",
    "\t\t\tWHEN job_title like '%insight%' THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%reporting%' THEN 'data analyst'\n",
    "\t\t\tWHEN job_title like '%model%' THEN 'data analyst'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%data%engineer%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%warehouse%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%architect%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%snowflake%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%etl%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%api%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%cloud%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%aws%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%kafka%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%pipeline%' THEN 'data engineer'\n",
    "\t\t\tWHEN job_title like '%migration%' THEN 'data engineer'\n",
    "\t\t\t\n",
    "\t\t\tWHEN job_title like '%scientist%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%science%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%machine%learning%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '% ai%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%ai %' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%ml%' THEN 'data scientist'\n",
    "\t\t\tWHEN job_title like '%computational%statistics%' then 'data scientist'\n",
    "\t\t\tWHEN job_title like '%quantitative%' then 'data scientist'\n",
    "\t\t\tWHEN job_title like '%artificial%intelligence%' then 'data scientist'\n",
    "\t\t\tEND AS job_standard_name\n",
    "            \n",
    "\tFROM dim_job_info\n",
    "\tORDER BY 2 DESC\n",
    "\t)\n",
    "\n",
    ", filtered_job_details AS (\n",
    "\tSELECT *\n",
    "\tFROM dim_job_details\n",
    "\tWHERE job_id IN (\n",
    "\t\tSELECT job_id FROM enriched \n",
    "-- \t\tWHERE job_standard_name = 'data analyst' )\n",
    "-- \t\tWHERE job_standard_name = 'data engineer' )\n",
    "\t\tWHERE job_standard_name = 'data scientist' )\n",
    ")\n",
    ", filtered_skills AS (\n",
    "\n",
    "\tSELECT job_id, 'SQL' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%sql%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Excel' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%excel%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'PowerBI' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%power%bi%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Tableau' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%tableau%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Python' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%python%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Cloud' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%cloud%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Azure' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%azure%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'AWS' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%aws%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'GCP' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%gcp%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'API' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%api%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Pipeline' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%pipeline%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Dimension Modelling' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%dimension%modelling%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'ETL' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%etl%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'ELT' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%elt%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'DevOps' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%devops%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'CI CD' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%ci%cd%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Spark' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%spark%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Java' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%java%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Scala' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%scala%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Oracle' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%oracle%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Kubernetes' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%kubernetes%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Docker' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%docker%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Apache' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%apache%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Kafka' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%kafka%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Linux' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%linux%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Snowflake' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%snowflake%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Warehouse' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%warehouse%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Modelling' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%modelling%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Visualisation' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%visualisation%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Migration' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%migration%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Management' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%management%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Integration' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%integration%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Platform' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%platform%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Architecture' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%architecture%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Factory' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%factory%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Databricks' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%databricks%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Data Science' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%data%science%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Machine Learning' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%machine%learning%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Computer Science' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%computer%science%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Research' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%research%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Statistic' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%statistic%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Mathematics' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%mathematics%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Quantitative' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%quantitative%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Algorithm' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%algorithm%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Deep Learning' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%deep%learning%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Statistical Analysis' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%statistical%analysis%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Communication Skill' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%communication%skill%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Stakeholder' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%stakeholder%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Reporting' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%reporting%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Agile' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%agile%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Project Management' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%project%management%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Business Intelligence' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%business%intelligence%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Decision Making' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%decision%making%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Interpersonal' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%interpersonal%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Time Management' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%time%management%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Troubleshoot' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%troubleshoot%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'Tertiary Qualification' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%tertiary%qualification%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    "\tUNION\n",
    "\tSELECT job_id, 'PhD' AS skill\n",
    "\t, MIN(line_id) AS line_id\n",
    "\tFROM filtered_job_details\n",
    "\tWHERE detail LIKE '%phd%'\n",
    "\tGROUP BY job_id\n",
    "\n",
    ")\n",
    "SELECT *\n",
    "\t, CASE \n",
    "\t\tWHEN line_id <= 3 THEN '1~3'\n",
    "\t\tWHEN (line_id > 3 AND line_id <= 5) THEN '4~5'\n",
    "\t\tELSE '6+' END AS line_id_category\n",
    "FROM filtered_skills\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
